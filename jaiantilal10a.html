 <head> 
  <link rel="alternate" type="application/rss+xml" href="http://jmlr.csail.mit.edu/jmlr.xml" title="JMLR RSS"> 
<link rel="stylesheet" type="text/css" href="http://jmlr.csail.mit.edu/style.css"> 
<style>. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style> 
<style type="text/css"> 
<!-- 
#fixed {
    position: absolute;
    top: 0;
    left: 0;
    width: 8em;
    height: 100%;
}
body > #fixed {
    position: fixed;
}
#content {
    margin-top: 1em;
    margin-left: 10em;
    margin-right: 0.5em;
}
img.jmlr {
    width: 7em;
}
img.rss {
    width: 2em;
}
-->
</style> 
<script LANGUAGE='JavaScript'> 
<!-- function GoAddress(user,machine) {
document.location = 'mailto:' + user + '@' + machine; } 
// -->
</script> 
 
 
<style> 
. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style> 
</head> 
 <body> 
 <div id="content"> 
<h2>Increasing Feature Selection Accuracy for L1 Regularized Linear Models</h2> 
<p><i><b>Abhishek Jaiantilal and Gregory Grudic</b></i>; 
JMLR W&P 10:86-96, 2010.</p> 
<h3>Abstract</h3> 
 
L1 (also referred to as the 1-norm or Lasso) penalty based formulations have been shown to be eective in problem domains when noisy features are present. However, the L1 penalty does not give favorable asymptotic properties with respect to feature selection, and has been shown to be inconsistent as a feature selection estimator; e.g. when noisy features are correlated with the relevant features. This can aect the estimation of the correct feature set, in certain domains like robotics, when both the number of examples and the number of features are large. The weighted lasso penalty by (Zou, 2006) has been proposed to rectify this problem of correct estimation of the feature set. This paper proposes a novel method for identifying problem specific L1 feature weights by utilizing the results from (Zou, 2006) and (Rocha et al., 2009) and is applicable to regression and classification algorithms. Our method increases the accuracy of L1 penalized algorithms through randomized experiments on subsets of the training data as a fast pre-processing step. We show experimental and theoretical results supporting the ecacy of the proposed method on two L1 penalized classification algorithms.
 
</div> 
 <div id="fixed"> 
<br> 
<a align="right" href="http://www.jmlr.org" target=_top><img align="right" class="jmlr" src="http://jmlr.csail.mit.edu/jmlr.jpg" border="0"></a> 
<p><br><br> 
<p align="right"> <A href="http://www.jmlr.org/"> Home Page </A> 
 
<p align="right"> <A href="/papers"> Papers </A> 
 
<p align="right"> <A href="/author-info.html"> Submissions </A> 
 
<p align="right"> <A href="/news.html"> News </A> 
 
<p align="right"> <A href="/scope.html"> Scope </A> 
 
<p align="right"> <A href="/editorial-board.html"> Editorial Board </A> 
 
<p align="right"> <A href="/announcements.html"> Announcements </A> 
 
<p align="right"> <A href="/proceedings"> Proceedings </A> 
 
<p align="right"> <A href="/mloss">Open Source Software</A> 
 
<p align="right"> <A href="/search-jmlr.html"> Search </A> 
 
<p align="right"> <A href="/manudb"> Login </A></p> 
 
<br><br> 
<p align="right"> <A href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="http://jmlr.csail.mit.edu/RSS.gif" class="rss" alt="RSS Feed"> 
</A> 
 
 
 
</div> 
 
</body>